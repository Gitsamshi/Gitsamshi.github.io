<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Zhan Shi's Personal Website</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0; /* Soft grey background */
            color: #333; /* Dark grey text */
            padding: 20px;
            max-width: 800px;
            margin: auto;
        }
        img.profile-photo {
            max-width: 100%;
            height: auto;
            border-radius: 50%;
        }
        h1, h2 {
            color: #007BFF; /* Casual blue for headings */
        }
        p, li {
            line-height: 1.6;
        }
        .section {
            background-color: #ffffff; /* White background for sections */
            padding: 20px;
            margin-top: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1); /* Subtle shadow for depth */
        }
    </style>
</head>
<body>
    <header>
        <img src="./19702190.jpeg" alt="Zhan Shi" style="width:200px;height:auto;">
        <h1>Zhan Shi</h1>
        <p>AI Researcher and Engineer | Specialized in NLP and Multi-Modality</p>
    </header>

    <div class="container">
        <div class="section about-me">
            <h2>About Me</h2>
            <p>AI researcher and engineer with first-author papers in top-tier NLP/multi-modality conferences (ACL, EMNLP, etc.).</p>
            <p>Adept at pre-training/finetuning optimization.</p>
            <p>Data expert in synthesizing dialogues with business SOPs.</p>
            <p><strong>Location:</strong> Seattle, WA</p>
            <p><strong>Email:</strong> <a href="mailto:shizhanszh34@gmail.com">shizhanszh34@gmail.com</a></p>
            <p><strong>Phone:</strong> (408) 504-9701</p>
            <p>Find me on <a href="https://www.linkedin.com/in/zshipassion/">LinkedIn</a> and <a href="https://scholar.google.com/citations?user=yROrWMkAAAAJ">Google Scholar</a>.</p>
        </div>

                <div class="section experience">
            <h2>Experience</h2>
            <h3>TikTok e-Commerce Machine Learning Engineer (April 2023 - Present)</h3>
            <ul>
                <li>Trained e-commerce multi-lingual foundational LLMs and built extensive benchmarks to evaluate the LLM's performance comprehensively.</li>
                <li>Developed e-commerce multi-modal foundation LLMs that support extensive tasks like product category prediction and brand identification.</li>
                <li>Launched a Product Description Page (PDP) Generative QA module; designed an RAG (Retrieval-Augmented Generation) module to address the hallucination problem in the Shopping Assistant chatbot.</li>
                <li>Created synthetic dialogue data that adheres to the standard operation procedure (SOP) for customer service, and trained/launched a multi-round customer service agent capable of using tools and generating responses.</li>
            </ul>

            <h3>Amazon Alexa AI Applied Scientist Intern (July 2022 - Oct 2022)</h3>
            <ul>
                <li>Investigated existing models by proposing novel dataset splits based on semantic and surface structure similarity, for debiased sentence representation learning.</li>
                <li>Implemented data augmentation techniques to counter bias while preserving word semantics, leveraging recall loss to prevent catastrophic forgetting, achieving state-of-the-art performance across STS benchmarks.</li>
            </ul>

            <h3>Samsung AI Center Research Intern (Mar 2022 – June 2022)</h3>
            <ul>
                <li>Proposed the concept of visual AMR (Abstract Meaning Representation) graphs focused on higher-level semantic concepts extrapolated from visual input, marking a significant step towards improved scene understanding.</li>
                <li>Developed meta-AMR graphs to unify information from multiple image descriptions under one representation, and trained a text-to-AMR parser to parse images into AMRs.</li>
            </ul>

            <h3>Baidu Research America Research Intern (June 2021 – Nov 2021)</h3>
            <ul>
                <li>Conducted robust constituency parsing and disfluency detection on erroneous transcripts from ASR (Automatic Speech Recognition) in real applications.</li>
                <li>Extended the evaluation metrics of parsing and disfluency detection from clean text to erroneous ASR output, augmenting the training data with a dynamic oracle for each ASR transcript, achieving significant improvements over baseline parsers.</li>
            </ul>

            <h3>Samsung Research America Research Intern (June 2020 – Sept 2020)</h3>
            <ul>
                <li>Presented a novel phrase grounding architecture leveraging external knowledge for further reasoning, significantly enhancing few-shot visual grounding capabilities.</li>
                <li>Constructed a multi-modal knowledge graph between image elements to infer spatial relations for target proposals based on the location of pre-detected objects.</li>
            </ul>

            <!-- Additional experiences can be detailed in a similar fashion -->
        </div>

        <div class="section education">
            <h2>Education</h2>
            <p>Ph.D. in Natural Language Processing, Queen’s University </p>
            <p>M.Sc. in Computer Science, Fudan University </p>
            <p>B.Eng. in Mechanical Engineering, Shandong University </p>
        </div>

        <div class="section publications">
            <h2>Selected Publications</h2>
            <ul>
                <li> <strong>Z Shi</strong>, G Wang, K Bai, J Li, X Li, Q Cui, B Zeng, T Chilimbi, X Zhu. 2023. OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding (EMNLP) </li>
                <li> MA Abdelsalam, <strong>Z Shi</strong>, F Fancellu, K Basioti, DJ Bhatt, A Fazly 2022. Visual Semantic Parsing: From Images to Abstract Meaning Representation (CoNLL) </li>
                <li> <strong>Z Shi</strong>, Y Shen, H Jin, X Zhu. 2022. Improving Zero-Shot Phrase Grounding via Reasoning on External Knowledge and Spatial Relations. (AAAI). </li>
                <li> <strong>Z Shi</strong>, H Liu, and X Zhu. 2021. Enhancing Descriptive Image Captioning with Natural Language Inference. (ACL). </li>
                <li> <strong>Z Shi</strong>, H Liu, MR Min, C Malon, LE Li, and X Zhu. 2021. Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning. (Findings of EMNLP). </li>
                <li> H Liu, <strong>Z Shi</strong>, and Xiaodan Zhu. 2021. Unsupervised Conversation Disentanglement through Co-Training. (EMNLP). </li>
                <li> <strong>Z Shi</strong>, X Zhou, X Qiu, and X Zhu. 2020. Improving Image Captioning with Better Use of Caption (ACL). </li>
                <li> H Liu, <strong>Z Shi</strong>, J Gu, Q Liu, S Wei, and X Zhu. 2020. End-to-End Transition-Based Online Dialogue Disentanglement (IJCAI). </li>
                <li> <strong>Z Shi</strong>, X Chen, X Qiu, and X Huang. 2018. Toward Diverse Text Generation with Inverse Reinforcement Learning. (IJCAI). </li>
                <li> X Chen, <strong>Z Shi</strong>, X Qiu, and X Huang. 2017. Adversarial Multi-criteria Learning for Chinese Word Segmentation. (ACL). (Outstanding Paper Award) </li>
                <!-- More publications listed similarly -->
            </ul>
        </div>

        <div class="section awards">
            <h2>Awards</h2>
            <p>ACL-2017 Outstanding Paper Award</p>
            <!-- More awards can be listed similarly -->
        </div>

        <div class="section skills">
            <h2>Skills</h2>
            <ul>
                <li>Python (PyTorch, TensorFlow, Pandas, NumPy)</li>
                <li>Java</li>
                <!-- More skills listed similarly -->
            </ul>
        </div>
    </div>

    <footer>
        <p>Contact me at <a href="mailto:shizhanszh34@gmail.com">shizhanszh34@gmail.com</a></p>
    </footer>
</body>
</html>
